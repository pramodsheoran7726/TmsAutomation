# Phase 1: Deep Scan & Context Building

> Systematically analyze every component of the TMS Automation framework to build a complete, indexed understanding before making any recommendations.

---

## Objective

Build a comprehensive context map of the entire framework â€” every file, every pattern, every dependency â€” so that subsequent phases can make informed, evidence-based decisions.

---

## ğŸš¨ CRITICAL RULES

```
âŒ DO NOT make any code changes in this phase.
âŒ DO NOT suggest improvements yet (that's Phase 2-3).
âŒ DO NOT skip any scan dimension.
âœ… DO read every relevant file thoroughly.
âœ… DO document findings with file paths and line numbers.
âœ… DO quantify everything (counts, sizes, percentages).
âœ… DO flag anomalies without proposing fixes.
```

---

## Scan Dimensions

Execute each scan dimension methodically. Use the Task tool with Explore subagents for parallel scanning where dimensions are independent.

### Dimension 1: Project Structure & Configuration

**What to scan:**
```
â”œâ”€â”€ package.json           â†’ Dependencies, scripts, versions
â”œâ”€â”€ playwright.config.ts   â†’ Projects, reporters, timeouts, workers
â”œâ”€â”€ tsconfig.json          â†’ Compiler options, path aliases, strictness
â”œâ”€â”€ .env / .env.example    â†’ Environment variables
â”œâ”€â”€ hyperexecute.yaml      â†’ CI cloud config
â”œâ”€â”€ .github/workflows/     â†’ All CI/CD pipelines
â””â”€â”€ .gitignore             â†’ What's tracked vs ignored
```

**Questions to answer:**
- Are all dependencies up to date? Any security vulnerabilities?
- Are TypeScript strict settings properly configured?
- Are path aliases used consistently across the codebase?
- Are generated directories (playwright-report/, allure-results/) in .gitignore?
- Do CI pipelines follow current best practices?
- Are environment variables documented and consistent?

**Output format:**
```markdown
### Configuration Assessment
| Config File | Status | Issues Found |
|-------------|--------|-------------|
| package.json | âœ…/âš ï¸/âŒ | Description |
```

---

### Dimension 2: Page Object Model Architecture

**What to scan:**
```
src/pages/
â”œâ”€â”€ {feature}/
â”‚   â”œâ”€â”€ {feature}.page.ts      â†’ Action methods
â”‚   â””â”€â”€ {feature}.locators.ts  â†’ Selector constants
â”œâ”€â”€ components/                 â†’ Reusable components
â”œâ”€â”€ common/                     â†’ Shared locators
â””â”€â”€ navigation/                 â†’ Navigation page
```

**Questions to answer:**
- Does every page follow the two-file pattern (*.page.ts + *.locators.ts)?
- Do all pages extend BasePage correctly?
- Are locators using resilient selectors (data-testid) or fragile ones (XPath text)?
- Are there duplicate locators across files?
- Are there unused locators?
- Do page methods use test.step() consistently?
- Are component classes (Toast, Delete, Search) reused properly?
- Is there a barrel export (index.ts) for pages?

**Locator Quality Assessment:**
```
For EACH locator file, classify selectors as:
- ğŸŸ¢ RESILIENT: data-testid, #id, [role="..."]
- ğŸŸ¡ MODERATE: CSS class, input[placeholder="..."]
- ğŸ”´ FRAGILE: XPath text match, deep nesting, positional

Calculate: Resilient% / Moderate% / Fragile% per file
```

**Output format:**
```markdown
### Page Object Assessment
| Module | Has Locators File | Has Page File | Extends BasePage | Uses test.step() | Locator Quality |
|--------|------------------|--------------|-----------------|------------------|----------------|
| project | âœ… | âœ… | âœ… | âš ï¸ 60% | ğŸŸ¡ 40/40/20 |
```

---

### Dimension 3: Test Specifications

**What to scan:**
```
tests/
â”œâ”€â”€ {feature}/
â”‚   â””â”€â”€ {feature}-{scenario}.spec.ts
```

**Questions to answer:**
- Does every test file follow naming conventions?
- Are tests using fixtures properly (from tms.fixture.ts)?
- Do tests have proper tags (@smoke, @regression)?
- Are tests independent (no shared state between tests)?
- Is test data generated fresh per test (random helpers)?
- Are assertions meaningful (not just toBeVisible)?
- Are there hardcoded values that should be constants?
- Is there proper cleanup (via fixtures or explicit)?
- Average test length (steps per test)?
- Are there any flaky patterns (bare waits, race conditions)?

**Flakiness Pattern Detection:**
```
Scan for these anti-patterns:
- page.waitForTimeout() â†’ Should use explicit waits
- Bare .click() without waiting for element â†’ Should use waitFor
- Fixed sleep values â†’ Should use network idle or DOM ready
- Assertions without timeout â†’ May fail on slow CI
- Tests dependent on execution order â†’ Should be independent
- Hardcoded URLs â†’ Should use EnvConfig
```

**Output format:**
```markdown
### Test Specification Assessment
| Feature | Specs | Avg Steps | Tags | Fixture Usage | Flaky Patterns | Quality |
|---------|-------|-----------|------|--------------|---------------|---------|
| test-run | 10 | 8 | âœ… | âœ… | 2 found | âš ï¸ |
```

---

### Dimension 4: Fixtures & Setup

**What to scan:**
```
src/fixtures/
â”œâ”€â”€ tms.fixture.ts       â†’ Main fixture definitions
â”œâ”€â”€ api.fixture.ts        â†’ API test fixtures
â””â”€â”€ api-setup.factory.ts  â†’ API setup with auto-cleanup
```

**Questions to answer:**
- How many fixtures are defined? Is the fixture file too large?
- Are fixtures properly typed (TmsFixtures interface)?
- Do composite fixtures handle cleanup correctly?
- Are there fixture dependencies that could cause deadlocks?
- Is the auth setup (auth.setup.ts) robust?
- Are API fixtures reusing authentication properly?
- Could fixtures be split into domain-specific files?

**Output format:**
```markdown
### Fixture Assessment
| Fixture | Type | Auto-Cleanup | Dependencies | Issues |
|---------|------|-------------|-------------|--------|
| projectOnly | Composite | âœ… | projectPage | None |
```

---

### Dimension 5: Utilities & Helpers

**What to scan:**
```
src/utils/
â”œâ”€â”€ base.page.ts      â†’ BasePage class
â”œâ”€â”€ api.helper.ts      â†’ HTTP request methods
â”œâ”€â”€ wait.helper.ts     â†’ Wait utilities
â”œâ”€â”€ retry.helper.ts    â†’ Retry logic
â”œâ”€â”€ random.helper.ts   â†’ Random generators
â”œâ”€â”€ date.helper.ts     â†’ Date utilities
â”œâ”€â”€ url.helper.ts      â†’ URL builders
â””â”€â”€ index.ts           â†’ Barrel exports
```

**Questions to answer:**
- Is BasePage in the right location (utils vs pages)?
- Are utility functions well-typed with generics?
- Is the retry helper used consistently across tests?
- Are wait helpers covering all necessary patterns?
- Are random generators producing unique enough values?
- Is there proper error handling in API helpers?
- Are there any utilities that should be extracted or consolidated?

---

### Dimension 6: API Layer

**What to scan:**
```
src/api/
â”œâ”€â”€ tms.api.ts    â†’ TMS backend client
â””â”€â”€ jira.api.ts   â†’ Jira integration client
```

**Questions to answer:**
- Are API methods properly typed (request/response)?
- Is error handling consistent?
- Are there hardcoded URLs that should use EnvConfig?
- Are authentication headers handled correctly?
- Is there proper logging for debugging API failures?
- Are API methods reusable across fixtures and tests?

---

### Dimension 7: Configuration & Constants

**What to scan:**
```
src/config/
â”œâ”€â”€ env.config.ts   â†’ Environment URL mappings
â””â”€â”€ constants.ts    â†’ Timeouts, routes, API paths
```

**Questions to answer:**
- Are all magic numbers extracted to constants?
- Are timeout values appropriate?
- Are route constants used consistently?
- Are API path constants matching actual endpoints?
- Is environment config validated at startup?
- Are there any config values that should be in .env?

---

### Dimension 8: Reporters & CI/CD

**What to scan:**
```
src/reporters/
â”œâ”€â”€ step-reporter.ts        â†’ Console reporter
â””â”€â”€ report-lab.reporter.ts  â†’ Dashboard reporter

.github/workflows/
â”œâ”€â”€ test.yml                â†’ Main pipeline
â”œâ”€â”€ us-tests.yml            â†’ US scheduled
â”œâ”€â”€ eu-tests.yml            â†’ EU scheduled
â””â”€â”€ hyperexecute.yml        â†’ Cloud execution

scripts/
â”œâ”€â”€ run-tests.js            â†’ CLI wrapper
â”œâ”€â”€ report-lab.ts           â†’ Report-Lab integration
â””â”€â”€ slack-notify.ts         â†’ Slack notifications
```

**Questions to answer:**
- Are reporters handling all edge cases (skipped, retried, timed out)?
- Are CI workflows DRY (shared steps vs duplicated)?
- Are there redundant scripts (run-tests.js vs pw.js)?
- Is the HyperExecute config optimized?
- Are Slack notifications informative?
- Are artifacts properly collected and retained?

---

### Dimension 9: TypeScript Quality & Code Readability

**What to scan:** ALL `.ts` files

**Questions to answer:**
- Are there any `any` types that should be properly typed?
- Are imports consistent (path aliases vs relative)?
- Are there unused imports or exports?
- Is `strict` mode fully leveraged?
- Are there type assertions (`as`) that indicate design issues?
- Are interfaces/types defined in `src/types/` and reused?

**Code Readability Assessment (scan a representative sample of 10 files):**
- Do exported functions have JSDoc comments?
- Are variable names descriptive (no single-letter or abbreviated names)?
- Are complex selectors/XPath expressions commented with what they match?
- Are imports organized (external â†’ alias â†’ relative with blank separators)?
- Are there commented-out code blocks that should be deleted?
- Do inline comments explain WHY (not WHAT)?
- Are functions focused (single responsibility) or doing too many things?
- Can a newcomer understand each file's purpose within 30 seconds?

**Readability Score:**
```
For each sampled file, rate:
- JSDoc coverage: X% of exports have JSDoc
- Comment quality: GOOD (WHY) / POOR (WHAT) / MISSING
- Naming quality: CLEAR / MIXED / CRYPTIC
- Structure clarity: CLEAN / ADEQUATE / CLUTTERED
```

---

### Dimension 10: Documentation & Maintainability

**What to scan:**
```
README.md
ARCHITECTURE.md
COVERAGE.md
COMPARISON.md
MIGRATION_REPORT.md
TEST-RESULTS.md
```

**Questions to answer:**
- Is the README accurate and useful for onboarding?
- Is ARCHITECTURE.md up to date?
- Are there stale documents that should be removed?
- Is there a CONTRIBUTING guide?
- Are there inline code comments where needed?
- Is the project self-documenting (good naming, clear structure)?

---

## Output: Scan Report

After completing ALL 10 dimensions, compile findings into `scan-report.md`:

```markdown
# Scan Report â€” TMS Automation Framework

## Executive Summary
- Total files scanned: X
- Total test specs: X
- Total page objects: X
- Total utilities: X
- Overall health score: X/100

## Dimension Scores
| Dimension | Score | Critical Issues | Warnings |
|-----------|-------|----------------|----------|
| 1. Configuration | X/10 | N | N |
| 2. Page Objects | X/10 | N | N |
| ...

## Critical Findings (must fix)
1. [Finding with file:line reference]

## Warnings (should fix)
1. [Finding with file:line reference]

## Observations (nice to have)
1. [Finding with file:line reference]

## Metrics
| Metric | Value |
|--------|-------|
| Test count | N |
| Page object count | N |
| Locator resilience | X% resilient |
| Fixture count | N |
| Average test steps | N |
| Flaky pattern count | N |
| TypeScript strictness | X% |
| Code duplication | X areas |
```

---

## ğŸ›‘ CHECKPOINT

After completing the scan report:

1. Display the **Executive Summary** and **Critical Findings** in chat
2. Save full report to `docs/tms-agent/maintenance-agent/runs/{timestamp}/scan-report.md`
3. **STOP and WAIT** for user to review before proceeding to Phase 2

---

## Execution Strategy

For efficiency, scan dimensions can be parallelized:

```
PARALLEL GROUP 1: Dimensions 1, 7, 10 (config & docs)
PARALLEL GROUP 2: Dimensions 2, 3 (pages & tests)
PARALLEL GROUP 3: Dimensions 4, 5, 6 (fixtures, utils, API)
PARALLEL GROUP 4: Dimensions 8, 9 (CI/CD & TypeScript quality)
```

Use the Task tool with Explore subagents for each parallel group.
